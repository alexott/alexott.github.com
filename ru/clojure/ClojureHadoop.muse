#title Использование Clojure с Hadoop
#keywords hadoop, clojure

Данная статья является небольшим введение в программирование Hadoop на Clojure.

<contents>

* Введение 

Проект [[http://hadoop.apache.org/][Hadoop]] является свободной реализацией инфраструктуры для распределенных,
масштабируемых вычислений.  Он начался как реализация идей MapReduce и GFS, введенных
Google, но со временем, в рамках проекат были реализованы и другие проекты.  Hadoop
активно используется во многих проектах, включая коммерческие -- Yahoo, LinkedIn, и т.д.,
и позволяет обрабатывать огромные объемы данных используя "стандартное" оборудование.

Hadoop написан на языке Java, что позволяет достаточно просто интегрировать его с Clojure.
Для упрощения программирования на Clojure для Hadoop, [[http://stuartsierra.com/][Stuart Sierra]] разработал пакет
=clojure-hadoop=, о котором и пойдет речь в данной статье.

* Установка и настройка 

Установка Hadoop в минимальной конфигурации, необходимой для наших экспериментов,
достаточно проста и описана в [[http://hadoop.apache.org/common/docs/current/quickstart.html][документации]].  Настройка Hadoop для работы в кластере
немного сложнее, но тоже [[http://hadoop.apache.org/common/docs/current/cluster_setup.html][описана достаточно подробно]].

Хочется отметить, что компания Cloudera [[http://www.cloudera.com/developers/downloads/][предоставляет готовые пакеты]] для разных версий
Linux, так что вы можете установить Hadoop и другие пакеты используя ваш любимый пакетный
менеджер.  Кроме того, они распространяют готовую к работе инсталяцию в виде образа для
VMWare, так что вы можете загрузить нужный архив и получить готовую рабочую среду.



http://github.com/stuartsierra/clojure-hadoop


examples - 
http://github.com/stuartsierra/clojure-hadoop/tree/master/src/examples/clojure/clojure_hadoop/examples/


* Из чего состоит clojure-hadoop

This library provides different layers of abstraction away from the
raw Hadoop API.

Layer 1: clojure-hadoop.imports

    Provides convenience functions for importing the many classes and
    interfaces in the Hadoop API.

Layer 2: clojure-hadoop.gen

    Provides gen-class macros to generate the multiple classes needed
    for a MapReduce job.  See the example file "wordcount1.clj" for a
    demonstration of these macros.

Layer 3: clojure-hadoop.wrap

    clojure-hadoop.wrap: provides wrapper functions that automatically
    convert between Hadoop Text objects and Clojure data structures.
    See the example file "wordcount2.clj" for a demonstration of these
    wrappers.

Layer 4: clojure-hadoop.job

    Provides a complete implementation of a Hadoop MapReduce job that
    can be dynamically configured to use any Clojure functions in the
    map and reduce phases.  See the example file "wordcount3.clj" for
    a demonstration of this usage.

Layer 5: clojure-hadoop.defjob

    A convenient macro to configure MapReduce jobs with Clojure code.
    See the example files "wordcount4.clj" and "wordcount5.clj" for
    demonstrations of this macro.



* Пример


Исходный код доступен на [[http://github.com/alexott/clojure-examples][github]], каталог =hadoop1=.

<src lang="clojure">
(ns hadoop1
    (:require [clojure-hadoop.wrap :as wrap]
              [clojure-hadoop.defjob :as defjob]
              [clojure-hadoop.imports :as imp])
    (:use clojure.contrib.seq-utils)
    (:require [clojure.contrib.str-utils2 :as str2])
    (:import (java.util StringTokenizer)))

(imp/import-io)
(imp/import-mapred)

(def delimiters "0123456789[ \t\n\r!\"#$%&'()*+,./:;<=>?@\\^`{|}~-]+")

(defn gen-n-grams [#^String s #^Integer n]
  (when (> (.length s) 0)
      (let [str (str " " s (String. ) (str2/repeat " " (- n 1)))]
        (reduce (fn [val elem]
                  (conj val (.substring str elem (+ elem n))))
                []
                (range 0 (+ 1 (.length s)))))))

(defn my-map [key #^String value]
  (map (fn [token] [token 1])
       (flatten (map #(gen-n-grams %1 3)
                     (enumeration-seq (StringTokenizer. value delimiters))))))

(defn my-reduce [key values-fn]
  [[key (reduce + (values-fn))]])

(defn string-long-writer [#^OutputCollector output
                          #^String key value]
  (.collect output (Text. key) (LongWritable. value)))

(defn string-long-reduce-reader [#^Text key wvalues]
  [(.toString key)
   (fn [] (map (fn [#^LongWritable v] (.get v))
               (iterator-seq wvalues)))])

(defjob/defjob job
  :map my-map
  :map-reader wrap/int-string-map-reader
  :map-writer string-long-writer
  :reduce my-reduce
  :reduce-reader string-long-reduce-reader
  :reduce-writer string-long-writer
  :output-key Text
  :output-value LongWritable
  :input-format :text
  :output-format :text
  :compress-output false)
</src>

Данный код основан на примере [[http://github.com/stuartsierra/clojure-hadoop/blob/master/src/examples/clojure/clojure_hadoop/examples/wordcount5.clj][wordcount5]] из поставки пакета clojure-hadoop. Для сборки
кода используется следующий проект Leiningen:

<src lang="clojure">
(defproject hadoop1 "1.0"
  :description "hadoop1"
  :dependencies [[org.clojure/clojure "1.1.0"]
                 [org.clojure/clojure-contrib "1.1.0"]
                 [com.stuartsierra/clojure-hadoop "1.2.0-SNAPSHOT"]]
  )
</src>

Для запуска проекта необходимо собрать все библиотеки в один архив, поэтому для сборки
должна использоваться команда =lein uberjar=, которая упакует код на Clojure, вместе со
всеми необходимыми библиотеками.  После сборки вы можете запустить задачу на выполнение
либо в автономном режиме -- без запущенного Hadoop, и работающего с локальными файлами[1],
либо в кластерном режиме.

Запуск в автономном режиме производится следующей командой:

<src lang="sh">
java -cp hadoop1-standalone.jar clojure_hadoop.job -job hadoop1/job -input FILE -output out
</src>

в качестве аргумента =FILE= подставьте нужный текстовый файл, и после выполнения программы,
в каталоге =out= будет создан файл, содержащий список N-Gram и их количество в тексте.
Данный файл можно затем использовать в качестве базы данных для задач определения языков и
т.п.

Для запуска вашего кода в кластере Hadoop вы должны поместить анализируемые файлы на HDFS
(в каталог =input=, в нашем примере), и запустить задачу на выполнение с помощью следуюшей
команды:

<src lang="sh">
hadoop jar hadoop1-standalone.jar clojure_hadoop.job -job hadoop1/job -input input -output output
</src>

После окончания выполнения задания, данные будут помещены в каталог =output=[2] на HDFS, и
вы можете получить доступ к ним используя стандартные команды работы с HDFS.

#more-info
* Заключение

Данная статья является лишь введением в использование Clojure для программирования на базе
Hadoop.  Дополнительные примеры применения clojure-hadoop вы можете найти в следующих
материалах: 
 - [[http://vimeo.com/7669741][Видео-презентация о Clojure и Hadoop на HadoopWorld NYC]] ([[http://stuartsierra.com/2009/10/02/clojurehadoop-slides][слайды]]);
 - [[http://www.bestinclass.dk/index.php/2010/01/hadoop-feeding-reddit-to-hadoop/][Hadoop - Feeding Reddit to Hadoop]] -- заметка в блоге, показывающая использование 
   Clojure & Hadoop для анализа данных, собранных с Reddit.

Дополнительная информация о программировании Hadoop и Map/Reduce вы можете найти в
следующих материалах:
 - [[http://www.cloudera.com/hadoop-training-programming-with-hadoop][Видео-лекция о программировании с помощью Hadoop]];
 - [[http://www.cloudera.com/hadoop-training-basic][Учебные курсы по работе с Hadoop созданные компаниией Cloudera]]
 - [[http://hadoop.apache.org/common/docs/current/mapred_tutorial.html][Map/Reduce Tutorial]]
 - [[http://www.amazon.com/gp/product/0596521979?ie=UTF8&amp;tag=aleottshompag-20&amp;linkCode=as2&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0596521979][Hadoop: The Definitive Guide]] -- отличная книга о работе и программировании Hadoop, и
   сопутствующищих проектов -- Pig, HBase, и других;
 - [[http://www.amazon.com/gp/product/1608453421?ie=UTF8&amp;tag=aleottshompag-20&amp;linkCode=as2&amp;camp=1789&amp;creative=390957&amp;creativeASIN=1608453421][Data-Intensive Text Processing with MapReduce]] -- книга об использовании Map/Reduce для
   анализа больших объемов текстовых данных, включая примеры использования Hadoop.




 - добавить описание http://github.com/nathanmarz/cascalog ?


;  LocalWords:  Cloudera Hadoop clojure hadoop VMWare GFS HDFS

Footnotes: 
[1] Запуск в автономном режиме удобен для отладки вашего кода, поскольку он выполняется в
    отдельном инстансе JVM, и работает с локальными файлами.

[2] Каталог для результатов не должен существовать, задание просто не запустится если
    каталог уже существует.

